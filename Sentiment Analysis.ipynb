{"cells":[{"cell_type":"markdown","source":["# Tweets Dataset\nIn this example, we use the sentiment140 corpus, comprised of about 1.6M tweets, each with the following information:\n- Polarity: 0 = negative, 2 = neutral, 4 = positive\n- ID\n- Date\n- Query\n- User\n- Text\n\nWe also used the AFINN wordlist which provides a **sentiment score** for 2476 English words to help our classifier achieve better accuracy. Each row of this dataset is:\n- Word\n- Score"],"metadata":{}},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# read raw data directly into spark\n#df_tweets = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/pw4anhpx1491487613286/\")\n\n# if data is small enough, use pandas\ndf_tweets = pd.read_csv(\"/dbfs/FileStore/tables/pw4anhpx1491487613286/training_1600000_processed_noemoticon-efba6.csv\", \n                  header=None, \n                  names=[\"polarity\", \"id\", \"date\", \"query\", \"user\", \"text\"])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# read raw data directly into spark\n#df_wordlist = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/pw4anhpx1491487613286/\")\n\n# if data is small enough, use pandas\ndf_wordlist = pd.read_csv(\"/dbfs/FileStore/tables/pw4anhpx1491487613286/AFINN_111-47bc9.txt\", \n                 sep=\"\\t\",\n                 header=None, \n                 names=[\"word\", \"score\"])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["# Feature Generation\nWe represent each teet as follows:\n- A TF-IDF vector of words from the text of the tweet\n- A positive and negative sentiment score, based on the sentiment wordlist\n- Temporal features including month, day-of-week, and hour-of-day"],"metadata":{}},{"cell_type":"markdown","source":["### Define Spark UDFs"],"metadata":{}},{"cell_type":"code","source":["import re, string\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import StringType, ArrayType, FloatType\n\n# Define PySpark UDF to tokenize text into words with various other specialized processing\npunct = re.compile('[%s]' % re.escape(string.punctuation))\n\ndef tok_str(text, ngrams=1, minChars=2):\n  # change any whitespace to regular space\n  text = re.sub(r'\\s+', ' ', text)\n  \n  # split into tokens and change to lower case\n  tokens = map(unicode, text.lower().split(' '))\n  \n  # remove short words and usernames\n  tokens = filter(lambda x: len(x) >= minChars and x[0] != '@', tokens)\n  \n  # replace any url by the constant word \"URL\"\n  tokens = [\"URL\" if t[:4] == \"http\" else t for t in tokens]\n  \n  # remove punctuation from tokens\n  tokens = [punct.sub('', t) for t in tokens]\n  \n  if ngrams==1:\n    return tokens\n  else:\n    return tokens + [' '.join(tokens[i:i+ngrams]) for i in xrange(len(tokens) - ngrams + 1)]\n  \ntokenize = F.udf(lambda s: tok_str(unicode(s), ngrams=2), ArrayType(StringType()))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["wordlist = dict([(r[0], r[1]) for r in df_wordlist.iterrows()])\n\n# Define PySpark UDF to get sentiment score using word-list\ndef pscore(words):\n  scores = filter(lambda x: x > 0, [wordlist[t] for t in words if t in wordlist])\n  return 0.0 if len(scores) == 0 else (float(sum(scores)) / len(scores))\n\npos_score = F.udf(lambda w: pscore(w), FloatType())\n\ndef nscore(words):\n  scores = filter(lambda x: x < 0, [wordlist[t] for t in words if t in wordlist])\n  return 0.0 if len(scores) == 0 else (float(sum(scores)) / len(scores))\n\nneg_score = F.udf(lambda w: nscore(w), FloatType())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Transform Data"],"metadata":{}},{"cell_type":"code","source":["df_tweets.head()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df_tweets_selected = df_tweets.loc[:, ['text', 'query', 'polarity', 'date']]\n\nhour_criteria = '([0-9]{2}):([0-9]{2}):([0-9]{2})'\ndayofweek_criteria = '(Sun|Mon|Tue|Wed|Thu|Fri|Sat)'\nmonth_criteria = '(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)'\n\ndf_tweets_selected['hour'] = [re.search(hour_criteria, x).group(1) for x in df_tweets_selected['date']]\ndf_tweets_selected['dayofweek'] = [re.search(dayofweek_criteria, x).group(1) for x in df_tweets_selected['date']]\ndf_tweets_selected['month'] = [re.search(month_criteria, x).group(1) for x in df_tweets_selected['date']]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["tw = sqlContext.createDataFrame(df_tweets_selected)\ntw.show(5)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# filter out tweets with neutral sentiment\ntw2 = tw.filter(\"polarity != 2\").withColumn('words', tokenize(tw['text']))\n\n# use tokenize UDF to transform text into bag of words\ntw3 = (tw2.select(\"hour\", \"dayofweek\", \"month\", \"words\",\n                 F.when(tw2.polarity == 4, \"Pos\").otherwise(\"Neg\").alias(\"sentiment\"),\n                 pos_score(tw2[\"words\"]).alias(\"pscore\"),\n                 neg_score(tw2[\"words\"]).alias(\"nscore\")))\n\n# store resulting feature matrix in Spark SQL temporary table\ntw3.registerTempTable(\"fm\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["# Train Classifier"],"metadata":{}},{"cell_type":"markdown","source":["### Pipeline"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, IDF, RegexTokenizer, HashingTF\nfrom pyspark.ml import Pipeline\n\n# parameters for modeling\nnumFeatures = 5000\nminDocFreq = 50\nnumTrees = 100\n\n# transform string variables to categorical variables\ninx1 = StringIndexer(inputCol=\"hour\", outputCol=\"hour-inx\")\ninx2 = StringIndexer(inputCol=\"month\", outputCol=\"month-inx\")\ninx3 = StringIndexer(inputCol=\"dayofweek\", outputCol=\"dow-inx\")\ninx4 = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n\n# compute TF-IDF on the words list for each tweer\nhashingTF = HashingTF(numFeatures=numFeatures, inputCol=\"words\", outputCol=\"hash-tf\")\nidf = IDF(minDocFreq=minDocFreq, inputCol=\"hash-tf\", outputCol=\"hash-tfidf\")\n\n# combines all features into a single feature vector\nva = VectorAssembler(inputCols=[\"hour-inx\", \"month-inx\", \"dow-inx\", \"hash-tfidf\", \"pscore\", \"nscore\"], outputCol=\"features\")\n\n# train using random forest algorithm\nrf = RandomForestClassifier(numTrees=numTrees, maxDepth=4, maxBins=32, labelCol=\"label\", seed=42)\n\n# build ML pipeline\np = Pipeline(stages=[inx1, inx2, inx3, inx4, hashingTF, idf, va, rf])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Modeling"],"metadata":{}},{"cell_type":"code","source":["# train test split\n(trainSet, testSet) = sqlContext.table(\"fm\").randomSplit([0.7, 0.3])\n\ntrainData = trainSet.cache()\ntestData = testSet.cache()\n\nmodel = p.fit(trainData)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{}},{"cell_type":"code","source":["def eval_metrics(lap):\n  tp = float(len(lap[(lap['label'] == 1) & (lap['prediction'] == 1)]))\n  tn = float(len(lap[(lap['label'] == 0) & (lap['prediction'] == 0)]))\n  fp = float(len(lap[(lap['label'] == 0) & (lap['prediction'] == 1)]))\n  fn = float(len(lap[(lap['label'] == 1) & (lap['prediction'] == 0)]))\n  precision = tp / (tp + fp)\n  recall = tp / (tp + fn)\n  accuracy = (tp + tn) / (tp + tn + fp + fn)\n  return {'precision': precision, 'recall': recall, 'accuracy': accuracy}"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Predict using test data\nresults = model.transform(testData)\nlap = results.select(\"label\", \"prediction\").toPandas()\nm = eval_metrics(lap)\n\nprint(m)"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"Sentiment Analysis","notebookId":3988106769761182},"nbformat":4,"nbformat_minor":0}
